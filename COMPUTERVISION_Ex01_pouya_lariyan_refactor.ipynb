{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lariyan.pouya\\\\.keras\\\\datasets\\\\images_background_extracted'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip\"\n",
    "path_to_zip = tf.keras.utils.get_file(\"images_background.zip\", origin=url, extract=True)\n",
    "path_to_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 105  # Image size for resizing\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 0, Total labels: 0\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Skipped: {image_path} is a directory.\")\n",
    "        return None  # Skip directories\n",
    "    \n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def get_image_paths_and_labels(base_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.png') or file.endswith('.jpg'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "                labels.append(root.split('/')[-2])  # Character as label\n",
    "    return image_paths, labels\n",
    "\n",
    "# Update the path to your dataset directory\n",
    "base_dir = \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/train\"\n",
    "image_paths, labels = get_image_paths_and_labels(base_dir)\n",
    "print(f\"Total images: {len(image_paths)}, Total labels: {len(set(labels))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740881052.509260    8704 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-02 05:34:12.549197: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(image_paths, labels):\n",
    "    images = []\n",
    "    valid_paths = []\n",
    "    for path in image_paths:\n",
    "        img = load_image(path)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            valid_paths.append(path)\n",
    "    return np.array(images), valid_paths\n",
    "\n",
    "X, valid_paths = prepare_data(image_paths, labels)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform([path.split('/')[-3] for path in valid_paths])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(images, labels):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    # Create pairs\n",
    "    for idx in range(len(images)):\n",
    "        current_img = images[idx]\n",
    "        current_label = labels[idx]\n",
    "        \n",
    "        # Positive pair\n",
    "        pos_idx = np.random.choice(np.where(labels == current_label)[0])\n",
    "        pos_img = images[pos_idx]\n",
    "        pairs.append([current_img, pos_img])\n",
    "        pair_labels.append(1)\n",
    "        \n",
    "        # Negative pair\n",
    "        neg_label = (current_label + np.random.randint(1, num_classes)) % num_classes\n",
    "        neg_idx = np.random.choice(np.where(labels == neg_label)[0])\n",
    "        neg_img = images[neg_idx]\n",
    "        pairs.append([current_img, neg_img])\n",
    "        pair_labels.append(0)\n",
    "    \n",
    "    return np.array(pairs), np.array(pair_labels)\n",
    "\n",
    "pairs, pair_labels = create_pairs(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.GlobalAveragePooling2D()\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_network():\n",
    "    feature_extractor = build_feature_extractor()\n",
    "    \n",
    "    input_a = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input_b = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    feat_a = feature_extractor(input_a)\n",
    "    feat_b = feature_extractor(input_b)\n",
    "    \n",
    "    # Compute L1 distance between features\n",
    "    l1_layer = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "    l1_distance = l1_layer([feat_a, feat_b])\n",
    "    \n",
    "    # Fully connected layer for similarity score\n",
    "    output = layers.Dense(1, activation='sigmoid')(l1_distance)\n",
    "    \n",
    "    siamese_network = models.Model(inputs=[input_a, input_b], outputs=output)\n",
    "    siamese_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return siamese_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = pairs[:, 0]\n",
    "X_b = pairs[:, 1]\n",
    "\n",
    "X_a = np.stack(X_a, axis=0)\n",
    "X_b = np.stack(X_b, axis=0)\n",
    "\n",
    "X_train_a, X_val_a, X_train_b, X_val_b, y_train, y_val = train_test_split(X_a, X_b, pair_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/anaconda3/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_7', 'keras_tensor_8']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 288ms/step - accuracy: 0.5029 - loss: 0.6915 - val_accuracy: 0.5550 - val_loss: 0.6685\n",
      "Epoch 2/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.5794 - loss: 0.6658 - val_accuracy: 0.6282 - val_loss: 0.6498\n",
      "Epoch 3/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.6282 - loss: 0.6387 - val_accuracy: 0.6239 - val_loss: 0.6268\n",
      "Epoch 4/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 287ms/step - accuracy: 0.6320 - loss: 0.6351 - val_accuracy: 0.6853 - val_loss: 0.5955\n",
      "Epoch 5/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 285ms/step - accuracy: 0.6530 - loss: 0.6077 - val_accuracy: 0.6832 - val_loss: 0.5973\n",
      "Epoch 6/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 283ms/step - accuracy: 0.6878 - loss: 0.5887 - val_accuracy: 0.7177 - val_loss: 0.5800\n",
      "Epoch 7/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 293ms/step - accuracy: 0.6926 - loss: 0.5802 - val_accuracy: 0.7177 - val_loss: 0.5667\n",
      "Epoch 8/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 294ms/step - accuracy: 0.7086 - loss: 0.5748 - val_accuracy: 0.7209 - val_loss: 0.5569\n",
      "Epoch 9/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 277ms/step - accuracy: 0.7315 - loss: 0.5407 - val_accuracy: 0.7414 - val_loss: 0.5418\n",
      "Epoch 10/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.7310 - loss: 0.5401 - val_accuracy: 0.7414 - val_loss: 0.5410\n",
      "Epoch 11/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 286ms/step - accuracy: 0.7335 - loss: 0.5371 - val_accuracy: 0.7069 - val_loss: 0.5653\n",
      "Epoch 12/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.7417 - loss: 0.5169 - val_accuracy: 0.7328 - val_loss: 0.5412\n",
      "Epoch 13/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 282ms/step - accuracy: 0.7628 - loss: 0.5083 - val_accuracy: 0.7478 - val_loss: 0.5203\n",
      "Epoch 14/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.7620 - loss: 0.5013 - val_accuracy: 0.7543 - val_loss: 0.5181\n",
      "Epoch 15/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 291ms/step - accuracy: 0.7891 - loss: 0.4682 - val_accuracy: 0.7543 - val_loss: 0.5273\n",
      "Epoch 16/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 288ms/step - accuracy: 0.7555 - loss: 0.5001 - val_accuracy: 0.7726 - val_loss: 0.4962\n",
      "Epoch 17/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 294ms/step - accuracy: 0.7781 - loss: 0.4708 - val_accuracy: 0.7554 - val_loss: 0.5036\n",
      "Epoch 18/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 293ms/step - accuracy: 0.7886 - loss: 0.4496 - val_accuracy: 0.7683 - val_loss: 0.4956\n",
      "Epoch 19/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 286ms/step - accuracy: 0.7916 - loss: 0.4518 - val_accuracy: 0.7726 - val_loss: 0.4728\n",
      "Epoch 20/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 286ms/step - accuracy: 0.8192 - loss: 0.4040 - val_accuracy: 0.7791 - val_loss: 0.4630\n",
      "Epoch 21/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 283ms/step - accuracy: 0.8124 - loss: 0.4217 - val_accuracy: 0.7597 - val_loss: 0.4823\n",
      "Epoch 22/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 291ms/step - accuracy: 0.8289 - loss: 0.3957 - val_accuracy: 0.7759 - val_loss: 0.4661\n",
      "Epoch 23/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 296ms/step - accuracy: 0.8384 - loss: 0.3712 - val_accuracy: 0.7662 - val_loss: 0.4823\n",
      "Epoch 24/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 291ms/step - accuracy: 0.8278 - loss: 0.3893 - val_accuracy: 0.7877 - val_loss: 0.4502\n",
      "Epoch 25/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 289ms/step - accuracy: 0.8410 - loss: 0.3666 - val_accuracy: 0.7963 - val_loss: 0.4392\n",
      "Epoch 26/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 296ms/step - accuracy: 0.8584 - loss: 0.3492 - val_accuracy: 0.7909 - val_loss: 0.4586\n",
      "Epoch 27/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 292ms/step - accuracy: 0.8551 - loss: 0.3379 - val_accuracy: 0.7963 - val_loss: 0.4313\n",
      "Epoch 28/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 286ms/step - accuracy: 0.8705 - loss: 0.3242 - val_accuracy: 0.7812 - val_loss: 0.4648\n",
      "Epoch 29/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 302ms/step - accuracy: 0.8698 - loss: 0.3193 - val_accuracy: 0.8017 - val_loss: 0.4394\n",
      "Epoch 30/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 289ms/step - accuracy: 0.8797 - loss: 0.3060 - val_accuracy: 0.7920 - val_loss: 0.4446\n",
      "Epoch 31/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 294ms/step - accuracy: 0.8859 - loss: 0.2900 - val_accuracy: 0.7974 - val_loss: 0.4348\n",
      "Epoch 32/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 289ms/step - accuracy: 0.8922 - loss: 0.2856 - val_accuracy: 0.8028 - val_loss: 0.4395\n",
      "Epoch 33/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 279ms/step - accuracy: 0.8918 - loss: 0.2833 - val_accuracy: 0.8103 - val_loss: 0.4328\n",
      "Epoch 34/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 278ms/step - accuracy: 0.8993 - loss: 0.2726 - val_accuracy: 0.8017 - val_loss: 0.4467\n",
      "Epoch 35/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 278ms/step - accuracy: 0.8991 - loss: 0.2618 - val_accuracy: 0.8039 - val_loss: 0.4351\n",
      "Epoch 36/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 278ms/step - accuracy: 0.9123 - loss: 0.2434 - val_accuracy: 0.8060 - val_loss: 0.4451\n",
      "Epoch 37/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 293ms/step - accuracy: 0.9189 - loss: 0.2353 - val_accuracy: 0.7996 - val_loss: 0.4431\n",
      "Epoch 38/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 290ms/step - accuracy: 0.9222 - loss: 0.2232 - val_accuracy: 0.8114 - val_loss: 0.4427\n",
      "Epoch 39/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 285ms/step - accuracy: 0.9197 - loss: 0.2239 - val_accuracy: 0.7899 - val_loss: 0.4606\n",
      "Epoch 40/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.9127 - loss: 0.2222 - val_accuracy: 0.8125 - val_loss: 0.4343\n",
      "Epoch 41/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 300ms/step - accuracy: 0.9232 - loss: 0.2121 - val_accuracy: 0.8039 - val_loss: 0.4514\n",
      "Epoch 42/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 294ms/step - accuracy: 0.9392 - loss: 0.1883 - val_accuracy: 0.7856 - val_loss: 0.5081\n",
      "Epoch 43/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 287ms/step - accuracy: 0.9295 - loss: 0.2014 - val_accuracy: 0.8006 - val_loss: 0.4522\n",
      "Epoch 44/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 285ms/step - accuracy: 0.9351 - loss: 0.1830 - val_accuracy: 0.8028 - val_loss: 0.4854\n",
      "Epoch 45/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 285ms/step - accuracy: 0.9292 - loss: 0.1988 - val_accuracy: 0.7996 - val_loss: 0.4628\n",
      "Epoch 46/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 295ms/step - accuracy: 0.9366 - loss: 0.1756 - val_accuracy: 0.7942 - val_loss: 0.4686\n",
      "Epoch 47/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 291ms/step - accuracy: 0.9386 - loss: 0.1696 - val_accuracy: 0.8006 - val_loss: 0.5000\n",
      "Epoch 48/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 287ms/step - accuracy: 0.9433 - loss: 0.1560 - val_accuracy: 0.8017 - val_loss: 0.4949\n",
      "Epoch 49/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 286ms/step - accuracy: 0.9515 - loss: 0.1480 - val_accuracy: 0.8017 - val_loss: 0.5019\n",
      "Epoch 50/50\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 285ms/step - accuracy: 0.9519 - loss: 0.1484 - val_accuracy: 0.8050 - val_loss: 0.5192\n"
     ]
    }
   ],
   "source": [
    "siamese_network = build_siamese_network()\n",
    "\n",
    "history = siamese_network.fit(\n",
    "    [X_train_a, X_train_b], y_train,\n",
    "    validation_data=([X_val_a, X_val_b], y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.7996 - loss: 0.5377\n",
      "Validation Loss: 0.5191612839698792, Validation Accuracy: 0.8049569129943848\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = siamese_network.evaluate([X_val_a, X_val_b], y_val)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "siamese_network.save(\"siamese_cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### inference ########\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "NUM_WAYS = 3     # 3 classes per episode\n",
    "NUM_SHOTS = 2    # 2 examples per class for support set\n",
    "NUM_QUERIES = 3  # 1 query example per class (you'll use your 3 new images)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "siamese_net = siamese_network\n",
    "\n",
    "\n",
    "# Manually selected paths for the support set (2 images per class, 3 classes)\n",
    "support_image_paths = [\n",
    "    # Class 0 images / Balinese\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Balinese/character01/0108_01.png\",\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Balinese/character01/0108_02.png\",\n",
    "\n",
    "    # Class 1 images /  Bengali\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Bengali/character08/0139_01.png\",\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Bengali/character08/0139_12.png\",\n",
    "\n",
    "    # Class 2 images / Blackfoot\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Blackfoot_(Canadian_Aboriginal_Syllabics)/character04/0181_04.png\",\n",
    "    \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Blackfoot_(Canadian_Aboriginal_Syllabics)/character06/0183_06.png\"\n",
    "]\n",
    "\n",
    "# Corresponding labels for the support set\n",
    "support_labels = [\n",
    "    0, 0,  # Class 0\n",
    "    1, 1,  # Class 1\n",
    "    2, 2   # Class 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Images Shape: (6, 105, 105, 3), Labels Shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def prepare_manual_support_set(support_image_paths, support_labels):\n",
    "    support_images = [load_and_preprocess(img) for img in support_image_paths]\n",
    "    support_images = np.stack(support_images)\n",
    "    support_labels = np.array(support_labels)\n",
    "    return support_images, support_labels\n",
    "\n",
    "support_images, support_labels = prepare_manual_support_set(support_image_paths, support_labels)\n",
    "print(f\"Support Images Shape: {support_images.shape}, Labels Shape: {support_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Blackfoot_(Canadian_Aboriginal_Syllabics)/character04/0181_05.png', '/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Bengali/character08/0139_03.png', '/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Balinese/character01/0108_08.png']\n",
      "Query Images Shape: (3, 105, 105, 3)\n"
     ]
    }
   ],
   "source": [
    "# Manually selected paths for the query set (3 new images to classify)\n",
    "query_image_paths = [\n",
    "    \n",
    "    # Class 2 images / Blackfoot\n",
    "   \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Blackfoot_(Canadian_Aboriginal_Syllabics)/character04/0181_05.png\",\n",
    "\n",
    "   # Class 1 images / Bengali\n",
    "   \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Bengali/character08/0139_03.png\",\n",
    "\n",
    "   \n",
    "   # Class 0 images / Balinese\n",
    "   \"/home/amin/Desktop/fsl/datasets/omniglot_extracted/images_background/val/Balinese/character01/0108_08.png\"\n",
    "   \n",
    "\n",
    "]\n",
    "\n",
    "def prepare_query_images(query_image_paths):\n",
    "    print(query_image_paths)\n",
    "    query_images = [load_and_preprocess(img) for img in query_image_paths]\n",
    "    return np.stack(query_images)\n",
    "\n",
    "query_images = prepare_query_images(query_image_paths)\n",
    "print(f\"Query Images Shape: {query_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Image 1: Predicted Class -> 1\n",
      "Image 2: Predicted Class -> 0\n",
      "Image 3: Predicted Class -> 1\n"
     ]
    }
   ],
   "source": [
    "def classify_images(support_images, support_labels, query_images, model):\n",
    "    predictions = []\n",
    "    for query_img in query_images:\n",
    "        query_img = tf.expand_dims(query_img, axis=0)  # Expand dims for batch compatibility\n",
    "        # Repeat query image for each support image\n",
    "        tiled_query = np.tile(query_img, (support_images.shape[0], 1, 1, 1))\n",
    "        # Predict similarity scores\n",
    "        scores = model.predict([tiled_query, support_images])\n",
    "        \n",
    "        # Aggregate scores per class\n",
    "        aggregated_scores = np.zeros(NUM_WAYS)\n",
    "        for i in range(NUM_WAYS):\n",
    "            aggregated_scores[i] = np.mean(scores[i * NUM_SHOTS:(i + 1) * NUM_SHOTS])\n",
    "        \n",
    "        predicted_class = np.argmax(aggregated_scores)\n",
    "        predictions.append(predicted_class)\n",
    "    return predictions\n",
    "\n",
    "predictions = classify_images(support_images, support_labels, query_images, siamese_net)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Image {i + 1}: Predicted Class -> {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
